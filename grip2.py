# -*- coding: utf-8 -*-
"""Grip2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dDnbZindHESDsYP2poIWpMsayi3fXw9A
"""

#Author: Sreya K

#Task #2: Prediction using unsupervised machine learning.

#GRIP@ The Sparks Foundation

#Problem statement: From the given "Iris" dataset,predict the optimum number of clusters and rerpresent it visually.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets
from sklearn.metrics import silhouette_score
import warnings as wg
wg.filterwarnings('ignore')
from pylab import rcParams

data=pd.read_csv(r"/content/Iris (3).csv")
data.head(10)#printing head of the data

data.shape

#checking for null values
data.isnull().sum()

data.info()#checking the shape of data

#finding statistical properties of the data
data.describe()

iris=pd.DataFrame(data)
iris_df=iris.drop(columns=["Species","Id"])#dropping the columns of Id and Species
#displaying the rearranged data
print(iris_df.head())
print(iris_df.tail())

#checking for outliers by boxplot
plt.rcParams["figure.figsize"]=(10,8)
iris_df.plot(kind="box")
plt.show()

#removing the ouliers using IQR method
Q1=iris_df.quantile(0.25)
Q3=iris_df.quantile(0.75)
IQR=Q3-Q1
iris_df=iris_df[~((iris_df<(Q1-1.5*IQR)) | (iris_df>(Q3+1.5*IQR))).any(axis=1)]

#plotting the boxplot after removing the outliers
plt.rcParams["figure.figsize"]=(10,8)
iris_df.plot(kind="box")
plt.show()

"""Now our data is free of outliers.

Step-6:- K-Means clustering

Let us perform the centroid-based clustering algorithm calles K-Means clustering. Such algorithms are efficient but sensitive to initial conditions and outliers. Here we consider two techniques- the Elbow method and the silhouette score method to decide on the optimum number of clusters(k) to perform the K-Means clustering
"""

x=iris_df.iloc[:,[0,1,2,3]].values #data arrangement

from sklearn.cluster import KMeans
#create several cluster combinations and observe the wcss(Within Cluster Sum of Squares)
wcss=[] #empty list to store the wcss
K=range(1,11)
for i in K:
    kmeans=KMeans(n_clusters=i,init='k-means++',
                  max_iter=300,n_init=10,random_state=0)
    kmeans=kmeans.fit(x)  #fit the model on prepared data
    wcss.append(kmeans.inertia_) #returns wcss for specific value of k
wcss

#visualize the elbow plot to find the optimal value of k
plt.plot(K,wcss,"go--")
#setting the plot title and axis labels
plt.title("The Elbow Method for optimal K")
plt.xlabel("Number of clusters")
plt.ylabel("Within Clusters Sum of Squares(WCSS)")
plt.annotate("Elbow",xytext=(4,200),xy=(2.4,120),arrowprops={"facecolor":"blue"})
plt.grid()
plt.show()#display the plot

"""Elbow plot is plotted with the value of k on the x-axis and the WCSS(Within Clusters Sum of Squares) on the y-axis. The value of k corresponding to the Elbow point represents the optimal value for k.

In the above plot, the Elbow point is representated by the arrow and the elbow occurs by 3 points.

Hence, the Elbow method shows that optimum value for k is 3.

Optimum value of k using Silhouette plot
"""

# initialise kmeans
kmeans = [KMeans(n_clusters=k,random_state=42).fit(x) for k in range(2,11)]
s=[silhouette_score(x, model.labels_) 
   for model in kmeans[1:]

#plotting silhoutte visualiser
from yellowbrick.cluster import SilhouetteVisualizer

fig,ax = plt.subplots(2, 2, figsize=(15,8))
for i in [2, 3, 4, 5]:
    
    #Create KMeans instance for different number of clusters
    
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    km_labels=km.fit_predict(x)
    q, mod = divmod(i, 2)
    
    #Create SilhouetteVisualizer instance with KMeans instance
    #Fit the visualizer
    
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(x)

"""The Silhouette plots for 2,3,4,5 clusters is shown above.

The silhouette plot shows that the value of k is 4 and 5 is a bad pick, as all the points in the cluster are below-average silhouette scores.

Silhouette analysis is more ambivalent in deciding between 2 and 3. The thickness of the silhouette plot for value of k=2, the second cluster is bigger in size owing to the grouping of the 3 sub-clusters into one big cluster.

For value of k=3, all the plots are more or less of similar thickness and hence are of similar sizes,also silhouette scores are well above average and so can be considered as best ‘k’
"""

#plotting the silhouette score
plt.plot(range(3,11),s)
plt.xlabel("Values of K") 
plt.ylabel("Silhouette score") 
plt.title("Silhouette analysis For Optimal k")
plt.show

"""In the above plot, we see the optimum value of silhouette score is when value of k is 3. Hence, combining the above two plots, the silhouette method shows the optimum value of k is 3.

so, from above two techniques we choose the number of clusters as '3'.

Step-7:- Visualizing the clusters.
"""

# Creating the kmeans classifier
kmeans=KMeans(n_clusters=3,init='k-means++',
                  max_iter=300,n_init=10,random_state=0)
y_kmeans=kmeans.fit_predict(x)   

y_kmeans

#Visualizing the clusters for the first two columns of the data
plt.scatter(x[ y_kmeans==0,0],x[ y_kmeans==0,1],s=75,c="red",label="Iris-setosa")
plt.scatter(x[ y_kmeans==1,0],x[ y_kmeans==1,1],s=75,c="orange",label="Iris-versicolor")
plt.scatter(x[ y_kmeans==2,0],x[ y_kmeans==2,1],s=75,c="yellow",label="Iris-virginica")
#Plotting centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=100,c="blue",label="Centroids")
plt.legend(loc=1,bbox_to_anchor=(1.35,1),prop={"size":15})
plt.show()

#Visualizing the clusters for the 3rd and 4th columns of the data
plt.scatter(x[ y_kmeans==0,2],x[ y_kmeans==0,3],s=75,c="red",label="Iris-setosa")
plt.scatter(x[ y_kmeans==1,2],x[ y_kmeans==1,3],s=75,c="orange",label="Iris-versicolor")
plt.scatter(x[ y_kmeans==2,2],x[ y_kmeans==2,3],s=75,c="yellow",label="Iris-virginica")
#Plotting centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:,2],kmeans.cluster_centers_[:,3],s=100,c="blue",label="Centroids")
plt.legend(loc=1,bbox_to_anchor=(1.35,1),prop={"size":15})
plt.show()

Thus, we have predicted the optimum number of clusters and represented it visually.

Thank you